version: '3.5'
x-airflow-common:
  &airflow-common
  build:
    context: docker
    dockerfile: airflow.Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-db/airflow
    _AIRFLOW_WWW_USER_USERNAME: 'admin'
    _AIRFLOW_WWW_USER_PASSWORD: 'admin'
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW_CONN_RDB_CONN: 'postgresql://dbapplication_user:dbapplication_user@rdb:5432/postgres'
    _PIP_ADDITIONAL_REQUIREMENTS: 'pandahouse==0.2.7 clickhouse-driver==0.2.1 apache-airflow-providers-slack'
  volumes:
    - /Users/khoa.nguyenkaligo.com/airflow/dags:/opt/airflow/dags
    - /Users/khoa.nguyenkaligo.com/airflow/logs:/opt/airflow/logs
    - /Users/khoa.nguyenkaligo.com/airflow/plugins:/opt/airflow/plugins
    - /Users/khoa.nguyenkaligo.com/airflow/data:/opt/airflow/data
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
  depends_on:
    - postgres-db

services:
  postgres-db:
    image: postgres:13.8-alpine
    container_name: postgres-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./postgres-db-volume:/var/lib/postgresql@13/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always
    ports:
      - 5432:5432

  af-websrv:
    container_name: af-websrv
    <<: *airflow-common
    command: webserver
    ports:
      - 28080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  af-sch:
    container_name: af-sch
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  af-trig:
    container_name: af-trig
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  af-int:
    container_name: af-int
    <<: *airflow-common
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'

  redis:
    image: redis:4.0-alpine
    ports:
      - 6379:6379
    command: redis-server --requirepass miniloyaltyengine
    volumes:
      - redis-volume:/data

  web:
    depends_on:
      - postgres-db
      - redis
    build: .
    command: >
      sh -c "bundle exec hanami db prepare
      && bundle exec hanami server --host 0.0.0.0 --server webrick"
    ports:
      - 2300:2300
    volumes:
      - .:/usr/src/app/
      - /Users/khoa.nguyenkaligo.com/.asdf/installs/ruby/2.7.6/lib/ruby/gems/2.7.0/gems/sidekiq-pro-5.5.5:/opt/airflow/data

  sidekiq:
    depends_on:
      - redis
      - web
    build: .
    command: sidekiq -e development -r ./config/environment.rb
    volumes:
      - .:/usr/src/app
    env_file:
      - .env.production

  sftp:
    image: atmoz/sftp
    volumes:
      - ./sftp-volume/upload:/home/miniloyaltyengine/upload
    ports:
      - "2222:22"
    command: miniloyaltyengine:pass:1001

volumes:
  postgres-db-volume:
  redis-volume:
  sftp-volume: